{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained optimization\n",
    "\n",
    "Julia has its own modeling language, `JuMP.jl`, kind of like GAMS or AMPL. If you write your problem using this interface, you can use GOBS of different solvers to solve your problem. Even better, Julia automatically cooks up derivatives using automagic differentiation (reverse mode, I think...). It also figures out what the sparsity patterns in your Hessian are, which is super nice, too. Unfortunately, the API isn't super-well documented and can be kind of tedious to use. The code below should help you if you want to use `JuMP` on the midterm or to code up BLP or whatever.\n",
    "\n",
    "JuMP: <https://jump.dev/JuMP.jl/stable/>\n",
    "\n",
    "JuMP guide on nonlinear programming (which is most of what we do): <https://jump.dev/JuMP.jl/stable/nlp/>\n",
    "\n",
    "MOI back end for JuMP: <https://jump.dev/MathOptInterface.jl/stable/>\n",
    "\n",
    "Ipopt: is an open-source constrained optimization software <https://coin-or.github.io/Ipopt/>. If performanced is an issue, you can look in to having Ipopt use a different solver for the sparse linear equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained optimiazion approach for Nonrenewable Rscs\n",
    "\n",
    "Here's a simple Hotelling model that you've seen earlier in lecture, and that you'll see again in ~~275~~ ARE 277\n",
    "\n",
    "$$\n",
    "\\max_{\\{q_t\\}_{t=0}^T} \\sum_{t=0}^T \\beta^t \\log q_t \\qquad st \\qquad Q \\geq \\sum_t q_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JuMP is Julia's linear + nonlinear programming language\n",
    "using JuMP\n",
    "\n",
    "# Ipopt is the \"free\" version of Knitro\n",
    "using Ipopt\n",
    "\n",
    "# for working w/ Hessian\n",
    "using LinearAlgebra, SparseArrays\n",
    "\n",
    "# Kind of cute if you want to make a nice table\n",
    "using DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "β = 0.9\n",
    "Q = 5.0\n",
    "\n",
    "model = Model(with_optimizer(Ipopt.Optimizer))  # define empty model solved by Ipopt algorithm\n",
    "@variable(model, q[i=1:n] >= 0)\n",
    "@constraint(model, sum(q[i] for i in 1:n) <= Q)\n",
    "@NLobjective(model, Max, sum(log(q[i])*β^(i-1) for i in 1:n))\n",
    "\n",
    "@show optimize!(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can call JuMP.value with the variable name to output it\n",
    "value.(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the model so that we have\n",
    "\n",
    "$$\n",
    "\\max_{\\{q_t,R_{t+1}\\}_{t=0}^T} \\sum_{t=0}^T \\beta^t \\left\\{q_t - c(q_t,R_t)\\right\\} \\qquad st \\qquad R_t - q_t \\geq R_{t+1}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 8\n",
    "β = 0.9\n",
    "Q = 10.0\n",
    "\n",
    "model = Model(with_optimizer(Ipopt.Optimizer))  # define empty model solved by Ipopt algorithm\n",
    "\n",
    "# define variables chosen by solver\n",
    "@variable(model, q[t=1:n] >= 0)\n",
    "@variable(model, R[t=1:n+1] >= 0)\n",
    "\n",
    "# Starting stock is constrained\n",
    "@constraint(model, R[1] == Q)\n",
    "\n",
    "# Stock transition\n",
    "@constraint(model, R[1:n] .- q[1:n] .>= R[2:n+1])\n",
    "\n",
    "# you can write \"expressions\" and re-use them\n",
    "@NLexpression(model, u[t=1:n], q[t] - 10*q[t]/R[t])\n",
    "\n",
    "# the objective function\n",
    "@NLobjective(model, Max, sum( u[t]*β^(t-1) for t in 1:n))\n",
    "\n",
    "# solve it\n",
    "@show optimize!(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame(q = vcat(value.(q),0), R=value.(R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the variables / values\n",
    "var_names = all_variables(model)\n",
    "var_hats = value.(var_names)\n",
    "DataFrame(nms = var_names, vals = var_hats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want to get gradient or Hessian, \n",
    "# need to use the MathOptInterface to get under the hood\n",
    "d = JuMP.NLPEvaluator(model)\n",
    "MOI.initialize(d, [:Grad, :Hess])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show nvar = num_variables(model)\n",
    "\n",
    "grad = zeros(nvar)\n",
    "MOI.eval_objective_gradient(d, grad, var_hats)\n",
    "grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation for the functions below is at\n",
    "\n",
    "<https://jump.dev/MathOptInterface.jl/stable/apireference/#NLP-evaluator-methods-1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure of hessian is sparse\n",
    "hess_sparsity = MOI.hessian_lagrangian_structure(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initalize\n",
    "H = zeros(length(hess_sparsity))\n",
    "\n",
    "# evaluate D²(f(θ) + ∑ᵢ λᵢ gᵢ(θ))\n",
    "λ = zeros(0)\n",
    "MOI.eval_hessian_lagrangian(d, H, var_hats, 1.0, λ)\n",
    "\n",
    "\"makes dense hessian from sparse lower-triangular hessian\"\n",
    "function dense_hessian(hessian_sparsity, V, n)\n",
    "    I = [i for (i,j) in hessian_sparsity]\n",
    "    J = [j for (i,j) in hessian_sparsity]\n",
    "    raw = sparse(I, J, V, n, n)\n",
    "    return Matrix(raw + raw' - sparse(diagm(0=>diag(raw))))\n",
    "end\n",
    "\n",
    "# dense Hessian\n",
    "hess = dense_hessian(hess_sparsity, H, nvar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0-beta1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
